{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import torch\n",
    "import gpytorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random \n",
    "from collections import namedtuple, deque "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(5*1e5)  #replay buffer size\n",
    "BATCH_SIZE = 4      # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3             # for soft update of target parameters\n",
    "LR = 1e-4            # learning rate\n",
    "UPDATE_EVERY = 4      # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPModelAlgorithm(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPModelAlgorithm, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.AdditiveKernel(\n",
    "                gpytorch.kernels.RBFKernel(),\n",
    "                gpytorch.kernels.MaternKernel(nu=0.5)  # Adjust nu for smoothness\n",
    "            )\n",
    "        )\n",
    "        self.linear_module = gpytorch.kernels.LinearKernel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x) + self.linear_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPModel():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.likelihood = None\n",
    "        self.predictions_lst = []\n",
    "\n",
    "    def train(self, X, y): #expects two tensors\n",
    "        # Set up the GP model with the new data\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        model = GPModelAlgorithm(X, y, likelihood)\n",
    "\n",
    "        # Use the entire training dataset to train the model\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "        \n",
    "        # Use ExactMarginalLogLikelihood to compute the loss\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        # Set up the optimizer and training loop\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "        num_epochs = 10\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = -mll(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        self.model = model\n",
    "        self.likelihood = likelihood\n",
    "    \n",
    "    def predict(self, X): #expects a tensor\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            predictions = self.likelihood(self.model(X))\n",
    "\n",
    "        # Extract predicted mean, lower, and upper bounds\n",
    "        mean = predictions.mean.numpy()\n",
    "        # lower, upper = predictions.confidence_region()\n",
    "        self.predictions_lst.append(mean)\n",
    "        return mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed -size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experiences = namedtuple(\"Experience\", field_names=[\"state\",\n",
    "                                                               \"action\",\n",
    "                                                               \"reward\",\n",
    "                                                               \"next_state\",\n",
    "                                                               \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self,state, action, reward, next_state,done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experiences(state,action,reward,next_state,done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory\"\"\"\n",
    "    \n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\" Actor (Policy) Model.\"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, fc1_unit=1024, fc2_unit = 1024):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model.\n",
    "        Params\n",
    "        =======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_unit (int): Number of nodes in first hidden layer\n",
    "            fc2_unit (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "\n",
    "        super(QNetwork,self).__init__() ## calls __init__ method of nn.Module class\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1= nn.Linear(state_size,fc1_unit)\n",
    "        self.fc2 = nn.Linear(fc1_unit,fc2_unit)\n",
    "        self.fc3 = nn.Linear(fc2_unit, action_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Build a network that maps state -> action values.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockEnv():\n",
    "    def __init__(self, trading_days, current_period, dates, stock_data, gp_model, available_money = 10000):\n",
    "\n",
    "        self.n_period = trading_days\n",
    "        self.current_period = current_period\n",
    "        self.stock_data = stock_data\n",
    "        self.dates = dates\n",
    "        self.available_money = available_money\n",
    "        self.gp_model = gp_model\n",
    "        self.invested = 0\n",
    "\n",
    "        self.gp_model.train(self.dates[:self.current_period+1], stock_data[:self.current_period+1])\n",
    "        prediction_price = self.gp_model.predict(dates[self.current_period : self.current_period+1])\n",
    "        \n",
    "        self.state = np.array([self.stock_data[self.current_period], prediction_price])\n",
    "\n",
    "        self.state_list = []\n",
    "        self.state_list.append(self.state)\n",
    "        self.action_list = []\n",
    "        self.reward_list = [] \n",
    "\n",
    "\n",
    "    def reset(self, trading_days, dates, stock_data, gp_model, available_money = 10000, current_period = 0):\n",
    "    \n",
    "        self.state_list = []\n",
    "        self.action_list = []\n",
    "        self.reward_list = []\n",
    "        self.n_period = trading_days\n",
    "        self.current_period = current_period\n",
    "        self.stock_data = stock_data\n",
    "        self.dates = dates\n",
    "        self.available_money = available_money\n",
    "        self.gp_model = gp_model\n",
    "        self.invested = 0\n",
    "\n",
    "        self.current_period = current_period\n",
    "\n",
    "        gp_model.train(dates[:current_period+1], stock_data[:current_period+1])\n",
    "        prediction_price = gp_model.predict(dates[current_period : current_period+1])\n",
    "\n",
    "        self.state = np.array([self.stock_data[current_period], prediction_price])\n",
    "        self.state_list.append(self.state)\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_period + 1 < self.n_period:\n",
    "            if action != 0: \n",
    "                reward = ((self.stock_data[self.current_period + 1] - self.stock_data[self.current_period])/self.stock_data[self.current_period]) * self.invested\n",
    "            if action == 0:\n",
    "                reward = 0\n",
    "            self.invested = (self.stock_data[self.current_period + 1]/self.stock_data[self.current_period]) * self.invested\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        if self.invested < 0:\n",
    "            self.invested = 0\n",
    "        self.current_period += 1\n",
    "        if self.current_period < self.n_period:\n",
    "            #this sohuld be current_period for train only\n",
    "            self.gp_model.train(self.dates[:self.current_period+1], self.stock_data[:self.current_period+1])\n",
    "            prediction_price = self.gp_model.predict(self.dates[self.current_period : self.current_period+1])\n",
    "            self.state = np.array([self.stock_data[self.current_period], prediction_price])\n",
    "        else:\n",
    "            prediction_price = 0\n",
    "            self.state = np.array([0,0])\n",
    "\n",
    "        self.state_list.append(self.state)\n",
    "\n",
    "        self.action_list.append(action)\n",
    "        self.reward_list.append(reward)\n",
    "\n",
    "        if self.current_period >= self.n_period:\n",
    "            terminate = True\n",
    "        else: \n",
    "            terminate = False\n",
    "        \n",
    "        if self.available_money ==0 and self.invested == 0:\n",
    "            terminate = True\n",
    "        return self.state, reward, terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(env, dates_tensor, data_tensor, gp_model, n_episodes= 10, max_t = 1000, eps_start=1.0, eps_end = 0.01, eps_decay=0.99):\n",
    "    \"\"\"Deep Q-Learning\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training epsiodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon \n",
    "        eps_decay (float): mutiplicative factor (per episode) for decreasing epsilon\n",
    "        \n",
    "    \"\"\"\n",
    "    scores = [] # list containing score from each episode\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset(trading_days = len(dates_tensor), dates = dates_tensor, stock_data = data_tensor, gp_model = gp_model, current_period = 0)\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "\n",
    "            action = agent.act(state, eps)\n",
    "            action = action - agent.action_size/2\n",
    "            if action > env.available_money and action > 0:\n",
    "                action = env.available_money\n",
    "\n",
    "            if action < 0 and np.abs(action) > env.invested:\n",
    "                action = -env.invested\n",
    "\n",
    "                            \n",
    "            env.available_money = env.available_money - action\n",
    "            env.invested = env.invested + action\n",
    "            \n",
    "            next_state,reward,done = env.step(action)\n",
    "\n",
    "            agent.step(state,action,reward,next_state,done)\n",
    "\n",
    "            ## above step decides whether we will train(learn) the network\n",
    "            ## actor (local_qnetwork) or we will fill the replay buffer\n",
    "            ## if len replay buffer is equal to the batch size then we will\n",
    "            ## train the network or otherwise we will add experience tuple in our \n",
    "            ## replay buffer.\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                print('episode '+str(i_episode)+' : ', score)\n",
    "                scores.append(score)\n",
    "                break\n",
    "\n",
    "        print('available money')\n",
    "        print(env.available_money)\n",
    "        print('invested money')\n",
    "        print(env.invested)\n",
    "        eps = max(eps*eps_decay,eps_end)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns form environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        =======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        \n",
    "        #Q- Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=LR)\n",
    "        \n",
    "        # Replay memory \n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_step, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_step, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step+1)% UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get radom subset and learn\n",
    "\n",
    "            if len(self.memory)>BATCH_SIZE:\n",
    "                experience = self.memory.sample()\n",
    "                self.learn(experience, GAMMA)\n",
    "        \n",
    "    def act(self, state, eps = 0):\n",
    "        \"\"\"Returns action for given state as per current policy\n",
    "        Params\n",
    "        =======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        #Epsilon -greedy action selction\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "            \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        =======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        ## TODO: compute and minimize the loss\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        # Local model is one which we need to train so it's in training mode\n",
    "        self.qnetwork_local.train()\n",
    "        # Target model is one with which we need to get our target so it's in evaluation mode\n",
    "        # So that when we do a forward pass with target model it does not calculate gradient.\n",
    "        # We will update target model weights with soft_update function\n",
    "        self.qnetwork_target.eval()\n",
    "        #shape of output from the model (batch_size,action_dim) = (64,4)\n",
    "\n",
    "        actions = actions + self.action_size/2\n",
    "        actions = actions.to(dtype=torch.int64)\n",
    "        predicted_targets = self.qnetwork_local(states).gather(1,actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            labels_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # .detach() ->  Returns a new Tensor, detached from the current graph.\n",
    "        labels = rewards + (gamma* labels_next*(1-dones))\n",
    "\n",
    "        loss = criterion(predicted_targets,labels).to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local,self.qnetwork_target,TAU)\n",
    "            \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        =======\n",
    "            local model (PyTorch model): weights will be copied from\n",
    "            target model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(),\n",
    "                                           local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch stock data\n",
    "def get_stock_data(symbol, start_date, end_date):\n",
    "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    stock_data = stock_data.resample('W').mean()\n",
    "    return stock_data['Open'], stock_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_symbol =  \"BTC-USD\"\n",
    "start_date = \"2014-09-17\"\n",
    "end_date = \"2023-12-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "btc_data, btc_dates = get_stock_data(btc_symbol, start_date, end_date)\n",
    "dates = np.arange(len(btc_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_tensor = torch.tensor(dates, dtype=torch.float32).view(-1)\n",
    "data_tensor = torch.tensor(btc_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasco\\AppData\\Roaming\\Python\\Python311\\site-packages\\gpytorch\\models\\exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode1: tensor(221238.8594)\n",
      "available money\n",
      "tensor(197067.)\n",
      "invested money\n",
      "tensor(52162.2422)\n",
      "episode2: tensor(566036.8125)\n",
      "available money\n",
      "tensor(28820.)\n",
      "invested money\n",
      "tensor(527778.4375)\n",
      "episode3: tensor(55036.1367)\n",
      "available money\n",
      "tensor(7662.)\n",
      "invested money\n",
      "tensor(67964.3125)\n",
      "episode4: tensor(614549.5000)\n",
      "available money\n",
      "tensor(123017.)\n",
      "invested money\n",
      "tensor(473222.9062)\n",
      "episode5: tensor(122152.2422)\n",
      "available money\n",
      "tensor(92607.7109)\n",
      "invested money\n",
      "tensor(3729.4622)\n",
      "episode6: tensor(152192.9062)\n",
      "available money\n",
      "tensor(7758.)\n",
      "invested money\n",
      "tensor(182199.8906)\n",
      "episode7: tensor(104858.1484)\n",
      "available money\n",
      "tensor(65305.4609)\n",
      "invested money\n",
      "tensor(50523.5000)\n",
      "episode8: tensor(88991.1172)\n",
      "available money\n",
      "tensor(33347.2344)\n",
      "invested money\n",
      "tensor(70906.8516)\n",
      "episode9: tensor(72164.6641)\n",
      "available money\n",
      "tensor(52609.0469)\n",
      "invested money\n",
      "tensor(29412.0742)\n",
      "episode10: tensor(91905.2812)\n",
      "available money\n",
      "tensor(30147.0312)\n",
      "invested money\n",
      "tensor(78241.2969)\n",
      "episode11: tensor(191441.0625)\n",
      "available money\n",
      "tensor(47048.)\n",
      "invested money\n",
      "tensor(157069.6406)\n",
      "episode12: tensor(47414.9062)\n",
      "available money\n",
      "tensor(48874.3164)\n",
      "invested money\n",
      "tensor(10385.6045)\n",
      "episode13: tensor(121261.9922)\n",
      "available money\n",
      "tensor(106095.)\n",
      "invested money\n",
      "tensor(22142.7480)\n",
      "episode14: tensor(100968.2734)\n",
      "available money\n",
      "tensor(55124.)\n",
      "invested money\n",
      "tensor(34106.5742)\n",
      "episode15: tensor(72539.7031)\n",
      "available money\n",
      "tensor(55643.6484)\n",
      "invested money\n",
      "tensor(28812.1641)\n",
      "episode16: tensor(77957.2656)\n",
      "available money\n",
      "tensor(42456.)\n",
      "invested money\n",
      "tensor(56480.6055)\n",
      "episode17: tensor(66643.2266)\n",
      "available money\n",
      "tensor(92691.2188)\n",
      "invested money\n",
      "tensor(15175.0430)\n",
      "episode18: tensor(209495.2812)\n",
      "available money\n",
      "tensor(28527.)\n",
      "invested money\n",
      "tensor(196514.2500)\n",
      "episode19: tensor(70749.1562)\n",
      "available money\n",
      "tensor(130409.6094)\n",
      "invested money\n",
      "tensor(3363.0486)\n",
      "episode20: tensor(154194.9531)\n",
      "available money\n",
      "tensor(87151.)\n",
      "invested money\n",
      "tensor(86663.2891)\n",
      "episode21: tensor(39984.1680)\n",
      "available money\n",
      "tensor(18786.3867)\n",
      "invested money\n",
      "tensor(57065.5625)\n",
      "episode22: tensor(70405.1094)\n",
      "available money\n",
      "tensor(3793.)\n",
      "invested money\n",
      "tensor(97471.3359)\n",
      "episode23: tensor(201484.3906)\n",
      "available money\n",
      "tensor(2172.)\n",
      "invested money\n",
      "tensor(247099.8125)\n",
      "episode24: tensor(15237.1289)\n",
      "available money\n",
      "tensor(24781.2773)\n",
      "invested money\n",
      "tensor(1722.0859)\n",
      "episode25: tensor(345843.5938)\n",
      "available money\n",
      "tensor(180361.)\n",
      "invested money\n",
      "tensor(144096.6406)\n",
      "episode26: tensor(110548.3438)\n",
      "available money\n",
      "tensor(128047.5391)\n",
      "invested money\n",
      "tensor(0.)\n",
      "episode27: tensor(22909.8828)\n",
      "available money\n",
      "tensor(16718.)\n",
      "invested money\n",
      "tensor(22347.8652)\n",
      "episode28: tensor(151259.1875)\n",
      "available money\n",
      "tensor(82627.)\n",
      "invested money\n",
      "tensor(85906.7422)\n",
      "episode29: tensor(156456.4219)\n",
      "available money\n",
      "tensor(68042.)\n",
      "invested money\n",
      "tensor(161686.5000)\n",
      "episode30: tensor(850813.7500)\n",
      "available money\n",
      "tensor(1972.)\n",
      "invested money\n",
      "tensor(980586.8125)\n",
      "episode31: tensor(306405.2500)\n",
      "available money\n",
      "tensor(92548.)\n",
      "invested money\n",
      "tensor(322372.0625)\n",
      "episode32: tensor(27133.2637)\n",
      "available money\n",
      "tensor(9943.)\n",
      "invested money\n",
      "tensor(38228.9141)\n",
      "episode33: tensor(2880.8340)\n",
      "available money\n",
      "tensor(13526.)\n",
      "invested money\n",
      "tensor(104.9604)\n",
      "episode34: tensor(169409.7188)\n",
      "available money\n",
      "tensor(30925.)\n",
      "invested money\n",
      "tensor(298589.7188)\n",
      "episode35: tensor(352015.5625)\n",
      "available money\n",
      "tensor(382334.)\n",
      "invested money\n",
      "tensor(604265.0625)\n",
      "episode36: tensor(59925.1445)\n",
      "available money\n",
      "tensor(46758.)\n",
      "invested money\n",
      "tensor(25199.9766)\n",
      "episode37: tensor(36495.6016)\n",
      "available money\n",
      "tensor(73843.8672)\n",
      "invested money\n",
      "tensor(4054.5918)\n",
      "episode38: tensor(75789.1172)\n",
      "available money\n",
      "tensor(52633.0938)\n",
      "invested money\n",
      "tensor(39752.4844)\n",
      "episode39: tensor(146728.1875)\n",
      "available money\n",
      "tensor(14068.)\n",
      "invested money\n",
      "tensor(94052.8516)\n",
      "episode40: tensor(63784.0078)\n",
      "available money\n",
      "tensor(59545.5625)\n",
      "invested money\n",
      "tensor(13804.6055)\n",
      "episode41: tensor(236548.5469)\n",
      "available money\n",
      "tensor(93830.3438)\n",
      "invested money\n",
      "tensor(138896.7188)\n",
      "episode42: tensor(72394.9062)\n",
      "available money\n",
      "tensor(16251.)\n",
      "invested money\n",
      "tensor(67348.8828)\n",
      "episode43: tensor(228764.5469)\n",
      "available money\n",
      "tensor(10680.)\n",
      "invested money\n",
      "tensor(175320.0312)\n",
      "episode44: tensor(438391.0625)\n",
      "available money\n",
      "tensor(1009.)\n",
      "invested money\n",
      "tensor(330992.5000)\n",
      "episode45: tensor(150916.6875)\n",
      "available money\n",
      "tensor(3305.)\n",
      "invested money\n",
      "tensor(558692.8125)\n",
      "episode46: tensor(1553429.6250)\n",
      "available money\n",
      "tensor(0.)\n",
      "invested money\n",
      "tensor(886936.3750)\n",
      "episode47: tensor(70883.8906)\n",
      "available money\n",
      "tensor(78754.7500)\n",
      "invested money\n",
      "tensor(1552.)\n",
      "episode48: tensor(125279.6172)\n",
      "available money\n",
      "tensor(147849.7969)\n",
      "invested money\n",
      "tensor(0.)\n",
      "episode49: tensor(203141.5469)\n",
      "available money\n",
      "tensor(20689.)\n",
      "invested money\n",
      "tensor(292033.7188)\n",
      "episode50: tensor(1044043.3125)\n",
      "available money\n",
      "tensor(0.)\n",
      "invested money\n",
      "tensor(1402045.)\n",
      "episode51: tensor(278024.9062)\n",
      "available money\n",
      "tensor(4228.)\n",
      "invested money\n",
      "tensor(330716.6875)\n",
      "episode52: tensor(47241.3672)\n",
      "available money\n",
      "tensor(70773.5547)\n",
      "invested money\n",
      "tensor(54.0696)\n",
      "episode53: tensor(84329.8594)\n",
      "available money\n",
      "tensor(105261.6172)\n",
      "invested money\n",
      "tensor(0.)\n",
      "episode54: tensor(37109.1211)\n",
      "available money\n",
      "tensor(45019.6875)\n",
      "invested money\n",
      "tensor(0.)\n",
      "episode55: tensor(56772.4648)\n",
      "available money\n",
      "tensor(53164.3477)\n",
      "invested money\n",
      "tensor(20576.7695)\n",
      "episode56: tensor(47423.9453)\n",
      "available money\n",
      "tensor(35240.5078)\n",
      "invested money\n",
      "tensor(26393.6875)\n",
      "episode57: tensor(50023.3086)\n",
      "available money\n",
      "tensor(62296.3828)\n",
      "invested money\n",
      "tensor(0.)\n",
      "episode58: tensor(10610.8027)\n",
      "available money\n",
      "tensor(16802.3027)\n",
      "invested money\n",
      "tensor(0.)\n",
      "episode59: tensor(23909.1367)\n",
      "available money\n",
      "tensor(0.)\n",
      "invested money\n",
      "tensor(25065.7969)\n",
      "episode60: tensor(543267.3750)\n",
      "available money\n",
      "tensor(2877.)\n",
      "invested money\n",
      "tensor(675437.1250)\n",
      "episode61: tensor(140029.3594)\n",
      "available money\n",
      "tensor(206693.5000)\n",
      "invested money\n",
      "tensor(0.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vasco\\Desktop\\Trabalhos Gui\\TDS publications\\Gaussian Processes\\RL+Gaussian_Process_btc.ipynb Cell 14\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m gp_model \u001b[39m=\u001b[39m GPModel()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m env \u001b[39m=\u001b[39m StockEnv(trading_days \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dates), current_period \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, dates \u001b[39m=\u001b[39m dates_tensor, stock_data \u001b[39m=\u001b[39m data_tensor, gp_model \u001b[39m=\u001b[39m gp_model, available_money \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m scores\u001b[39m=\u001b[39m dqn(env, dates_tensor \u001b[39m=\u001b[39;49m dates_tensor, data_tensor\u001b[39m=\u001b[39;49mdata_tensor, gp_model\u001b[39m=\u001b[39;49mgp_model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(scores)), scores)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mReward\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\vasco\\Desktop\\Trabalhos Gui\\TDS publications\\Gaussian Processes\\RL+Gaussian_Process_btc.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m env\u001b[39m.\u001b[39minvested \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39minvested \u001b[39m+\u001b[39m action\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m next_state,reward,done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m agent\u001b[39m.\u001b[39;49mstep(state,action,reward,next_state,done)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m## above step decides whether we will train(learn) the network\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m## actor (local_qnetwork) or we will fill the replay buffer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m## if len replay buffer is equal to the batch size then we will\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m## train the network or otherwise we will add experience tuple in our \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m## replay buffer.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n",
      "\u001b[1;32mc:\\Users\\vasco\\Desktop\\Trabalhos Gui\\TDS publications\\Gaussian Processes\\RL+Gaussian_Process_btc.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory)\u001b[39m>\u001b[39mBATCH_SIZE:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     experience \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39msample()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn(experience, GAMMA)\n",
      "\u001b[1;32mc:\\Users\\vasco\\Desktop\\Trabalhos Gui\\TDS publications\\Gaussian Processes\\RL+Gaussian_Process_btc.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m# ------------------- update target network ------------------- #\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vasco/Desktop/Trabalhos%20Gui/TDS%20publications/Gaussian%20Processes/RL%2BGaussian_Process_btc.ipynb#X16sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoft_update(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnetwork_local,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnetwork_target,TAU)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     adam(\n\u001b[0;32m    164\u001b[0m         params_with_grad,\n\u001b[0;32m    165\u001b[0m         grads,\n\u001b[0;32m    166\u001b[0m         exp_avgs,\n\u001b[0;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    169\u001b[0m         state_steps,\n\u001b[0;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m func(params,\n\u001b[0;32m    312\u001b[0m      grads,\n\u001b[0;32m    313\u001b[0m      exp_avgs,\n\u001b[0;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    316\u001b[0m      state_steps,\n\u001b[0;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:384\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    381\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[0;32m    383\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m exp_avg\u001b[39m.\u001b[39;49mlerp_(grad, \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[0;32m    385\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=2, action_size=20000,seed=0)\n",
    "gp_model = GPModel()\n",
    "\n",
    "env = StockEnv(trading_days = len(dates), current_period = 0, dates = dates_tensor, stock_data = data_tensor, gp_model = gp_model, available_money = 10000)\n",
    "\n",
    "scores= dqn(env, dates_tensor = dates_tensor, data_tensor=data_tensor, gp_model=gp_model)\n",
    "\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "torch.save(agent.qnetwork_local.state_dict(), 'path')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
